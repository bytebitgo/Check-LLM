import os
import streamlit as st
import asyncio
from components.model_selector import model_selector, model_parameters
from services import LLMServiceFactory
from services.base import Message
from services.openai_service import OpenAIService
from services.azure_openai_service import AzureOpenAIService
from services.anthropic_service import AnthropicService

def check_service_config(service_name: str) -> bool:
    """æ£€æŸ¥æœåŠ¡é…ç½®æ˜¯å¦å®Œæ•´"""
    if service_name == "OpenAI":
        return bool(os.getenv("OPENAI_API_KEY"))
    elif service_name == "Azure OpenAI":
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•Azure OpenAIé…ç½®ç»„
        env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')
        if os.path.exists(env_path):
            with open(env_path, 'r', encoding='utf-8') as f:
                current_group = None
                has_valid_config = False
                current_config = {}
                
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    if line.startswith('## '):
                        # æ£€æŸ¥ä¸Šä¸€ä¸ªé…ç½®ç»„æ˜¯å¦æœ‰æ•ˆ
                        if current_config and all(key in current_config for key in [
                            'AZURE_OPENAI_API_KEY',
                            'AZURE_OPENAI_ENDPOINT',
                            'AZURE_DEPLOYMENT_NAME'
                        ]):
                            has_valid_config = True
                            break
                        current_group = line[3:].strip()
                        current_config = {}
                        continue
                    if line.startswith('#'):
                        continue
                    if '=' in line and current_group:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip()
                        if key.startswith('AZURE_') and value and value != 'your_azure_openai_api_key':
                            current_config[key] = value
                
                # æ£€æŸ¥æœ€åä¸€ä¸ªé…ç½®ç»„
                if current_config and all(key in current_config for key in [
                    'AZURE_OPENAI_API_KEY',
                    'AZURE_OPENAI_ENDPOINT',
                    'AZURE_DEPLOYMENT_NAME'
                ]):
                    has_valid_config = True
                
                return has_valid_config
        return False
    elif service_name == "Anthropic":
        return bool(os.getenv("ANTHROPIC_API_KEY"))
    return False

async def get_service_provider():
    """è·å–æœåŠ¡æä¾›å•†"""
    # åˆå§‹åŒ–session state
    if 'provider' not in st.session_state:
        st.session_state.provider = None
    if 'model' not in st.session_state:
        st.session_state.model = None
    
    providers = {
        "openai": "OpenAI",
        "azure-openai": "Azure OpenAI",
        "anthropic": "Anthropic",
        "google": "Google"
    }
    
    # è·å–å·²é…ç½®çš„æœåŠ¡æä¾›å•†
    available_providers = {
        key: name for key, name in providers.items()
        if check_service_config(name)
    }
    
    if not available_providers:
        st.error("æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æœåŠ¡æä¾›å•†ï¼Œè¯·å…ˆåœ¨é…ç½®ç®¡ç†é¡µé¢è®¾ç½®ç›¸å…³APIå¯†é’¥")
        return None, None
    
    # é€‰æ‹©æœåŠ¡æä¾›å•†
    provider = st.selectbox(
        "é€‰æ‹©æœåŠ¡æä¾›å•†",
        options=list(available_providers.keys()),
        format_func=lambda x: available_providers[x]
    )
    
    if not provider:
        return None, None
    
    # å®ä¾‹åŒ–æœåŠ¡
    try:
        service = LLMServiceFactory.get_service(provider)
        models = await service.get_available_models()
        
        if not models:
            st.error(f"{providers[provider]} æœªè·å–åˆ°å¯ç”¨æ¨¡å‹ï¼Œè¯·æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®")
            return None, None
        
        model = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            options=models,
            format_func=lambda x: x.split(" - ")[-1] if " - " in x else x
        )
        
        return provider, model
        
    except Exception as e:
        st.error(f"åˆå§‹åŒ– {providers[provider]} æœåŠ¡å¤±è´¥: {str(e)}")
        return None, None

async def chat_page():
    """èŠå¤©é¡µé¢"""
    st.title("æ¨¡å‹å¯¹è¯æµ‹è¯•")
    
    try:
        # åˆå§‹åŒ–session state
        if "messages" not in st.session_state:
            st.session_state.messages = []
        if "performance_records" not in st.session_state:
            st.session_state.performance_records = []
        
        # ä¾§è¾¹æ é…ç½®
        with st.sidebar:
            st.header("æ¨¡å‹é…ç½®")
            provider, model = await get_service_provider()
            
            if not provider or not model:
                return
            
            # æ¸…ç©ºèŠå¤©æŒ‰é’®
            if st.button("æ¸…ç©ºèŠå¤©è®°å½•", type="secondary"):
                st.session_state.messages = []
                st.session_state.performance_records = []  # åŒæ—¶æ¸…ç©ºæ€§èƒ½è®°å½•
                st.rerun()
        
        # æ˜¾ç¤ºèŠå¤©å†å²
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # ç”¨æˆ·è¾“å…¥
        if prompt := st.chat_input():
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            st.session_state.messages.append({
                "role": "user",
                "content": prompt
            })
            
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # è°ƒç”¨API
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                stats_placeholder = st.empty()
                full_response = ""
                
                try:
                    message_placeholder.markdown("ğŸ¤” æ€è€ƒä¸­...")
                    
                    # è·å–æµå¼å“åº”
                    response_generator = await LLMServiceFactory.get_service(provider).chat_completion(
                        messages=[
                            {"role": msg["role"], "content": msg["content"]}
                            for msg in st.session_state.messages
                        ],
                        model=model,
                        stream=True
                    )
                    
                    # å¤„ç†æµå¼å“åº”
                    async for chunk in response_generator:
                        if chunk["type"] == "content":
                            full_response += chunk["content"]
                            message_placeholder.markdown(full_response + "â–Œ")
                        elif chunk["type"] == "stats":
                            # æ˜¾ç¤ºæœ€ç»ˆå“åº”ï¼ˆä¸å¸¦å…‰æ ‡ï¼‰
                            message_placeholder.markdown(full_response)
                            
                            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²è®°å½•
                            st.session_state.messages.append({
                                "role": "assistant",
                                "content": full_response
                            })
                            
                            # è·å–æ¨¡å‹ä¿¡æ¯ä»¥è®¡ç®—æˆæœ¬
                            model_info = await LLMServiceFactory.get_service(provider).get_model_info(model)
                            
                            if model_info and model_info.pricing:
                                # è®¡ç®—æˆæœ¬
                                prompt_cost = (
                                    model_info.pricing["input"] *
                                    chunk["stats"]["prompt_tokens"] / 1000
                                )
                                completion_cost = (
                                    model_info.pricing["output"] *
                                    chunk["stats"]["completion_tokens"] / 1000
                                )
                                total_cost = prompt_cost + completion_cost
                                
                                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                                stats_placeholder.caption(
                                    f"å“åº”æ—¶é—´: {chunk['stats']['response_time']:.2f}ç§’ | "
                                    f"è¾“å…¥tokens: {chunk['stats']['prompt_tokens']} | "
                                    f"è¾“å‡ºtokens: {chunk['stats']['completion_tokens']} | "
                                    f"æ€»tokens: {chunk['stats']['total_tokens']} | "
                                    f"æˆæœ¬: ${total_cost:.4f}"
                                )
                                
                                # æ·»åŠ æ€§èƒ½è®°å½•
                                performance_record = {
                                    "provider": provider,
                                    "model": model,
                                    "response_time": chunk["stats"]["response_time"],
                                    "prompt_tokens": chunk["stats"]["prompt_tokens"],
                                    "completion_tokens": chunk["stats"]["completion_tokens"],
                                    "total_tokens": chunk["stats"]["total_tokens"],
                                    "cost": total_cost
                                }
                                
                                if "performance_records" not in st.session_state:
                                    st.session_state.performance_records = []
                                st.session_state.performance_records.append(performance_record)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å“åº”
                    if not full_response:
                        message_placeholder.error("æ¨¡å‹æ²¡æœ‰ç”Ÿæˆä»»ä½•å“åº”")
                        if st.session_state.messages:
                            st.session_state.messages.pop()  # ç§»é™¤ç”¨æˆ·æ¶ˆæ¯
                        return
                    
                except Exception as e:
                    error_msg = str(e)
                    if "æœªæ”¶åˆ°æ¨¡å‹å“åº”" in error_msg:
                        message_placeholder.error("æ¨¡å‹æ²¡æœ‰ç”Ÿæˆå“åº”ï¼Œè¯·é‡è¯•")
                    else:
                        message_placeholder.error(f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {error_msg}")
                    
                    # ç§»é™¤ç”¨æˆ·æ¶ˆæ¯ï¼ˆä»…åœ¨å®Œå…¨å¤±è´¥æ—¶ï¼‰
                    if not full_response and st.session_state.messages:
                        st.session_state.messages.pop()
                
    except Exception as e:
        st.error(f"é¡µé¢åŠ è½½å‡ºé”™: {str(e)}") 